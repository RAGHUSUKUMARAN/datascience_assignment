{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Cardiotocographic Dataset\n",
    "\n",
    "This notebook performs EDA on the dataset located at `D:\\\\DATA SCIENCE\\\\ASSIGNMENTS\\\\5 EDA1\\\\EDA1\\\\Cardiotocographic.csv`.\n",
    "\n",
    "It includes:\n",
    "- Data loading & cleaning\n",
    "- Statistical summary\n",
    "- Outlier detection\n",
    "- Visualizations\n",
    "- Pattern recognition\n",
    "- Conclusion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset (Windows path)\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "input_path = r\"D:\\\\DATA SCIENCE\\\\ASSIGNMENTS\\\\5 EDA1\\\\EDA1\\\\Cardiotocographic.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Inspect dataset\n",
    "df.info()\n",
    "print(\"\\nShape:\", df.shape)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data cleaning\n",
    "# Remove duplicates\n",
    "dupes = df.duplicated().sum()\n",
    "if dupes > 0:\n",
    "    df = df.drop_duplicates()\n",
    "print(\"Duplicates dropped:\", dupes)\n",
    "\n",
    "# Convert to numeric where possible\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col].str.strip(), errors='coerce')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Handle missing values\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        if np.issubdtype(df[col].dtype, np.number):\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode().iloc[0], inplace=True)\n",
    "\n",
    "print(\"Missing values handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Outlier detection (IQR method)\n",
    "outlier_summary = {}\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "    outlier_summary[col] = {'lower': lower, 'upper': upper, 'outliers': outliers.shape[0]}\n",
    "outlier_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Statistical summary\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "summary = pd.DataFrame(index=num_cols)\n",
    "summary['count'] = df[num_cols].count()\n",
    "summary['mean'] = df[num_cols].mean()\n",
    "summary['median'] = df[num_cols].median()\n",
    "summary['std'] = df[num_cols].std()\n",
    "summary['IQR'] = df[num_cols].quantile(0.75) - df[num_cols].quantile(0.25)\n",
    "summary['skew'] = df[num_cols].skew()\n",
    "summary['kurtosis'] = df[num_cols].kurtosis()\n",
    "summary.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualizations\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Histograms\n",
    "df[num_cols].hist(figsize=(14, 12), bins=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Boxplots\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(num_cols[:6], 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr = df[num_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Pattern recognition\n",
    "corr_pairs = corr.unstack().sort_values(ascending=False)\n",
    "corr_pairs = corr_pairs[corr_pairs < 1].dropna().head(20)\n",
    "corr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save cleaned dataset to same Windows folder\n",
    "output_clean = r\"D:\\\\DATA SCIENCE\\\\ASSIGNMENTS\\\\5 EDA1\\\\EDA1\\\\Cardiotocographic_cleaned.csv\"\n",
    "df.to_csv(output_clean, index=False)\n",
    "print(\"Cleaned dataset saved to:\", output_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "- Dataset cleaned: missing values imputed, duplicates dropped, data types corrected.\n",
    "- Outliers flagged via IQR method; extreme cases need domain review.\n",
    "- Statistical summary highlighted skewness in FM and UC, and variability in ASTV/ALTV.\n",
    "- Visualizations confirmed skewness, presence of outliers, and correlations among variability features.\n",
    "- Cleaned dataset saved alongside original for reproducibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
